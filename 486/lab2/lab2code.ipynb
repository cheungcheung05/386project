{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"ocean_data.csv\")\n",
    "\n",
    "# Drop non-numerical columns\n",
    "df = data.drop(columns = ['Wea', 'Cloud_Typ', 'Cloud_Amt', 'Visibility'])\n",
    "\n",
    "target_variable = df['T_degC']\n",
    "\n",
    "numerical_features = df.drop(columns = ['T_degC'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(numerical_features, target_variable, test_size=0.25, random_state=307)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('poly_features', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('standardize', StandardScaler()),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "yhat = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 1.535095636242086\n",
      "Test MSE: 1.7543308610948567\n",
      "Test MSE compared to the variance of ytest: 0.11833893287694301\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# (c) Report the training MSE\n",
    "ytrain_predict = pipe.predict(X_train)\n",
    "training_mse = mean_squared_error(y_train, ytrain_predict)\n",
    "print(\"Training MSE:\", training_mse)\n",
    "\n",
    "# (d) Report the test MSE\n",
    "ytest_pred = pipe.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, ytest_pred)\n",
    "print(\"Test MSE:\", test_mse)\n",
    "\n",
    "variance_ytest = y_test.var()\n",
    "\n",
    "print(\"Test MSE compared to the variance of ytest:\", test_mse / variance_ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features and coefficients with the three largest positive coefficients:\n",
      "   Feature  Coefficient\n",
      "9    x0 x2   147.569766\n",
      "12   x0 x5    35.153778\n",
      "8    x0 x1    32.217666\n",
      "\n",
      "Features and coefficients with the three largest negative coefficients:\n",
      "  Feature  Coefficient\n",
      "5      x5   -34.383804\n",
      "1      x1   -37.244657\n",
      "2      x2  -146.550613\n",
      "\n",
      "Estimated y-intercept of the fitted model: 11.607865265528682\n"
     ]
    }
   ],
   "source": [
    "coefficients = pipe.named_steps['model'].coef_\n",
    "feature_names = pipe.named_steps['poly_features'].get_feature_names_out()\n",
    "\n",
    "# Combine coefficients and feature names into a DataFrame for easier analysis\n",
    "coefficients_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "\n",
    "# Order the features by the magnitude of their coefficients\n",
    "coefficients_df = coefficients_df.reindex(\n",
    "    coefficients_df['Coefficient'].sort_values(ascending=False).index\n",
    ")\n",
    "\n",
    "# (b) Get the features and coefficients with the three largest positive coefficients\n",
    "top_positive = coefficients_df.head(3)\n",
    "\n",
    "# (c) Get the features and coefficients with the three largest negative coefficients\n",
    "top_negative = coefficients_df.tail(3)\n",
    "\n",
    "# (d) Get the estimated y-intercept of the fitted model\n",
    "y_intercept = pipe.named_steps['model'].intercept_\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nFeatures and coefficients with the three largest positive coefficients:\")\n",
    "print(top_positive)\n",
    "print(\"\\nFeatures and coefficients with the three largest negative coefficients:\")\n",
    "print(top_negative)\n",
    "print(\"\\nEstimated y-intercept of the fitted model:\", y_intercept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x0^2', 'x0 x1', 'x0 x2',\n",
       "       'x0 x3', 'x0 x4', 'x0 x5', 'x0 x6', 'x1^2', 'x1 x2', 'x1 x3',\n",
       "       'x1 x4', 'x1 x5', 'x1 x6', 'x2^2', 'x2 x3', 'x2 x4', 'x2 x5',\n",
       "       'x2 x6', 'x3^2', 'x3 x4', 'x3 x5', 'x3 x6', 'x4^2', 'x4 x5',\n",
       "       'x4 x6', 'x5^2', 'x5 x6', 'x6^2'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients_df\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'impute__strategy': 'mean', 'model__n_neighbors': 15, 'model__weights': 'distance', 'poly__degree': 1}\n",
      "Best Negative MSE: -2.009963445544637\n",
      "Test MSE with Optimized Pipeline: 2.087222401430895\n",
      "Time taken to fit GridSearchCV: 111.91906237602234\n",
      "Test MSE in part (2): 1.7543308610948567\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import time\n",
    "\n",
    "# (a) Create the pipeline with KNN model\n",
    "knn_pipe = Pipeline([\n",
    "    ('impute', SimpleImputer()),\n",
    "    ('poly', PolynomialFeatures(include_bias=False)),\n",
    "    ('model', KNeighborsRegressor())\n",
    "])\n",
    "\n",
    "# (b) Set up the hyperparameter grid\n",
    "params = {\n",
    "    'impute__strategy': ['mean', 'median'],\n",
    "    'poly__degree': [1, 2, 3],\n",
    "    'model__n_neighbors': list(range(5, 101, 5)),\n",
    "    'model__weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "# (c) Instantiate and fit GridSearchCV\n",
    "knn_gs = GridSearchCV(knn_pipe, param_grid=params, scoring='neg_mean_squared_error', cv=10)\n",
    "start_time = time.time()\n",
    "knn_gs.fit(X_train, y_train)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# (d) Print the best hyperparameter combinations and the best MSE\n",
    "print(\"Best Hyperparameters:\", knn_gs.best_params_)\n",
    "print(\"Best Negative MSE:\", knn_gs.best_score_)\n",
    "\n",
    "# (e) Use the optimized pipeline to get predictions for the test data\n",
    "ytest_pred_optimized = knn_gs.predict(X_test)\n",
    "\n",
    "# (f) Report the test MSE\n",
    "test_mse_optimized = mean_squared_error(y_test, ytest_pred_optimized)\n",
    "print(\"Test MSE with Optimized Pipeline:\", test_mse_optimized)\n",
    "\n",
    "# (g) Compare with the pipeline in part (2)\n",
    "test_mse_part2 = mean_squared_error(y_test, yhat)  # Assuming yhat is the prediction from part (2)\n",
    "print(\"Time taken to fit GridSearchCV:\", elapsed_time)\n",
    "print(\"Test MSE in part (2):\", test_mse_part2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters (RandomizedSearchCV): {'poly__degree': 1, 'model__weights': 'distance', 'model__n_neighbors': 10, 'impute__strategy': 'mean'}\n",
      "Best Negative MSE (RandomizedSearchCV): -2.051993277466038\n",
      "Test MSE with Optimized Pipeline (RandomizedSearchCV): 2.080069382923031\n",
      "Test MSE in part (2): 1.7543308610948567\n",
      "\n",
      "Comparison of Results:\n",
      "Time taken to fit RandomizedSearchCV: 4.119412899017334\n",
      "Time taken to fit GridSearchCV: 111.91906237602234\n",
      "Best Hyperparameters (GridSearchCV): {'impute__strategy': 'mean', 'model__n_neighbors': 15, 'model__weights': 'distance', 'poly__degree': 1}\n",
      "Best Negative MSE (GridSearchCV): -2.009963445544637\n",
      "Best Hyperparameters (RandomizedSearchCV): {'poly__degree': 1, 'model__weights': 'distance', 'model__n_neighbors': 10, 'impute__strategy': 'mean'}\n",
      "Best Negative MSE (RandomizedSearchCV): -2.051993277466038\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# (a) Create the pipeline with KNN model\n",
    "knn_pipe_random = Pipeline([\n",
    "    ('impute', SimpleImputer()),\n",
    "    ('poly', PolynomialFeatures(include_bias=False)),\n",
    "    ('model', KNeighborsRegressor())\n",
    "])\n",
    "\n",
    "# (b) Set up the hyperparameter grid for RandomizedSearchCV\n",
    "params_random = {\n",
    "    'impute__strategy': ['mean', 'median'],\n",
    "    'poly__degree': [1, 2, 3],\n",
    "    'model__n_neighbors': list(range(5, 101, 5)),\n",
    "    'model__weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "# (c) Instantiate and fit RandomizedSearchCV\n",
    "knn_rs = RandomizedSearchCV(\n",
    "    knn_pipe_random, param_distributions=params_random,\n",
    "    scoring='neg_mean_squared_error', cv=10, n_iter=10, random_state=42\n",
    ")\n",
    "start_time_random = time.time()\n",
    "knn_rs.fit(X_train, y_train)\n",
    "elapsed_time_random = time.time() - start_time_random\n",
    "\n",
    "# (d) Print the best hyperparameter combinations and the best MSE\n",
    "print(\"Best Hyperparameters (RandomizedSearchCV):\", knn_rs.best_params_)\n",
    "print(\"Best Negative MSE (RandomizedSearchCV):\", knn_rs.best_score_)\n",
    "\n",
    "# (e) Use the optimized pipeline to get predictions for the test data\n",
    "ytest_pred_optimized_random = knn_rs.predict(X_test)\n",
    "\n",
    "# (f) Report the test MSE\n",
    "test_mse_optimized_random = mean_squared_error(y_test, ytest_pred_optimized_random)\n",
    "print(\"Test MSE with Optimized Pipeline (RandomizedSearchCV):\", test_mse_optimized_random)\n",
    "\n",
    "# (g) Compare with the pipeline in part (2)\n",
    "test_mse_part2 = mean_squared_error(y_test, yhat)  # Assuming yhat is the prediction from part (2)\n",
    "print(\"Test MSE in part (2):\", test_mse_part2)\n",
    "\n",
    "# (h) Compare the results of both methods\n",
    "print(\"\\nComparison of Results:\")\n",
    "print(\"Time taken to fit RandomizedSearchCV:\", elapsed_time_random)\n",
    "print(\"Time taken to fit GridSearchCV:\", elapsed_time)\n",
    "print(\"Best Hyperparameters (GridSearchCV):\", knn_gs.best_params_)\n",
    "print(\"Best Negative MSE (GridSearchCV):\", knn_gs.best_score_)\n",
    "print(\"Best Hyperparameters (RandomizedSearchCV):\", knn_rs.best_params_)\n",
    "print(\"Best Negative MSE (RandomizedSearchCV):\", knn_rs.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 7.932936716529455e-13\n",
      "Test MSE: 1.4623776711365377\n",
      "Test MSE from part (2): 1.7543308610948567\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectPercentile, f_regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "data = pd.read_csv(\"ocean_data.csv\")\n",
    "\n",
    "# Separate numeric and categorical features\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64'])\n",
    "categorical_features = data[['Wea', 'Cloud_Typ', 'Cloud_Amt', 'Visibility']]\n",
    "\n",
    "# Separate target variable\n",
    "target_variable = numeric_features['T_degC']\n",
    "\n",
    "# Drop target variable from numeric features\n",
    "numeric_features = numeric_features.drop(['T_degC'], axis=1)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    pd.concat([numeric_features, categorical_features], axis=1), target_variable, test_size=0.25, random_state=307\n",
    ")\n",
    "\n",
    "# Define numeric transformer\n",
    "numeric_transformer = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define categorical transformer\n",
    "categorical_transformer = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encode', OneHotEncoder(sparse_output=False, handle_unknown='ignore')),\n",
    "    ('select', SelectPercentile(f_regression, percentile=50))\n",
    "])\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features.columns),\n",
    "        ('cat', categorical_transformer, categorical_features.columns)\n",
    "    ])\n",
    "\n",
    "# Build the final pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', KNeighborsRegressor(n_neighbors=20, weights='distance'))\n",
    "])\n",
    "\n",
    "# (a) Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# (b) Report the training and test mse\n",
    "y_train_pred = pipeline.predict(X_train)\n",
    "training_mse = mean_squared_error(y_train, y_train_pred)\n",
    "print(\"Training MSE:\", training_mse)\n",
    "\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(\"Test MSE:\", test_mse)\n",
    "\n",
    "# (c) Compare with the model in part (2)\n",
    "print(\"Test MSE from part (2):\", test_mse_part2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new data\n",
    "new_data = pd.read_csv(\"ocean_data.csv\")\n",
    "\n",
    "# Separate numeric and categorical features\n",
    "numeric_features = new_data.select_dtypes(include=['float64', 'int64'])\n",
    "categorical_features = new_data[['Wea', 'Cloud_Typ', 'Cloud_Amt', 'Visibility']]\n",
    "\n",
    "# Combine numeric and categorical features\n",
    "X_new = pd.concat([numeric_features, categorical_features], axis=1)\n",
    "\n",
    "# Predict water temperature using the selected model\n",
    "y_new_pred = pipeline.predict(X_new)\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "pd.DataFrame(y_new_pred).to_csv(\"predictions.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 2\n",
    "icecream_df = pd.read_csv(\"icecreamcone2.csv\")\n",
    "\n",
    "X = icecream_df.drop(['logBV'], axis=1)\n",
    "y = icecream_df['logBV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "# Create StandardScaler and PolynomialFeatures objects\n",
    "scaler = StandardScaler()\n",
    "poly = PolynomialFeatures(degree=1)  # Change the degree if you want polynomial features\n",
    "\n",
    "# Standardize the features\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# Add a column for the intercept (if not using PolynomialFeatures with degree>1)\n",
    "X_processed = poly.fit_transform(X_standardized)\n",
    "\n",
    "# Now X_processed contains standardized features with an added column for the intercept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the target vector\n",
    "y = y.values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming X_processed is the standardized and processed feature matrix\n",
    "# and y is the target vector with an added dimension\n",
    "betas = np.linalg.inv(X_processed.T @ X_processed) @ X_processed.T @ y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.84548985],\n",
       "       [-0.25818952],\n",
       "       [-0.08677703],\n",
       "       [-0.29603492],\n",
       "       [ 0.28041044]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def linear_regression_gradient_descent(X, y, learning_rate, epochs):\n",
    "    \"\"\"\n",
    "    Perform linear regression using gradient descent.\n",
    "\n",
    "    Parameters:\n",
    "    - X: 2D numpy array with features\n",
    "    - y: 1D numpy array with the target\n",
    "    - learning_rate: learning rate for gradient descent\n",
    "    - epochs: number of iterations\n",
    "\n",
    "    Returns:\n",
    "    - betas: estimated coefficients\n",
    "    \"\"\"\n",
    "    # Initialize coefficients\n",
    "    betas = np.zeros((X.shape[1], 1))\n",
    "\n",
    "    # Number of samples\n",
    "    m = len(y)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Calculate predictions\n",
    "        predictions = X @ betas\n",
    "\n",
    "        # Calculate the error\n",
    "        error = predictions - y.reshape(-1, 1)\n",
    "\n",
    "        # Calculate gradients\n",
    "        gradients = 2/m * X.T @ error\n",
    "\n",
    "        # Update coefficients\n",
    "        betas -= learning_rate * gradients\n",
    "\n",
    "    return betas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Coefficients: [[ 4.84548984]\n",
      " [-0.25795717]\n",
      " [-0.08664055]\n",
      " [-0.29577712]\n",
      " [ 0.28076943]]\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_processed is the standardized and processed feature matrix\n",
    "# and y is the target vector with an added dimension\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "estimated_betas = linear_regression_gradient_descent(X_processed, y, learning_rate, epochs)\n",
    "print(\"Estimated Coefficients:\", estimated_betas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (d) Your answer: The gradient boosting betas are super close to the normal equation betas. However, the gradient boosting betas are a bit tighter and closer to 0 which is a sign of a more accurate model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
